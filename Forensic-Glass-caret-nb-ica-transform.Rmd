---
title: "Forensic Glass:  Naive Bayes Analysis"
output: html_document
---

efg, 2017-09-04

[NaiveBayes: Naive Bayes Classifier](https://rdrr.io/cran/klaR/man/NaiveBayes.html) using [klaR package](https://cran.r-project.org/web/packages/klaR/klaR.pdf) with [caret](http://topepo.github.io/caret/index.html).

With ICA transformation

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

```{r}
time.1 <- Sys.time()
```

### Required packages

```{r, comment=NA, message=FALSE, warning=FALSE}
library(MASS)          # fgl data
library(dplyr)         # select
library(caret)         # createDataParition, nearZeroVar
library(doParallel)    # registerDoParallel  
library(fastICA)
library(klaR)
```

### Forensic Glass Data

```{r, comment=NA}
rawData <- fgl
dim(rawData)
```

```{r, comment=NA}
table(rawData$type)
prop.table( table(rawData$type) )
```

### Caret info about Naive Bayes model

```{r, comment=NA}
getModelInfo()$nb$type
```

```{r, comment=NA}
getModelInfo()$nb$parameters
```

```{r, comment=NA}
getModelInfo()$nb$library
```

```{r, comment=NA}
getModelInfo()$nb$grid
```

### Define train and test datasets

```{r, comment=NA}
set.seed(71)

trainSetIndices <- createDataPartition(rawData$type, p=0.70, list=FALSE)

trainSet <- rawData[ trainSetIndices, ]
testSet  <- rawData[-trainSetIndices, ]
```

Assume dependent variable is last column of trainSet and testSet here.

```{r, comment=NA}
nrow(trainSet)
nrow(testSet)
```


```{r, comment=NA}
table(trainSet$type) 
table(testSet$type)
```


```{r, comment=NA}
prop.table( table(trainSet$type) )
prop.table( table(testSet$type) )
```

#### Remove near-zero variance predictors

```{r, comment=NA}
nearZero <- nearZeroVar(trainSet, saveMetrics=TRUE)
nearZero
```
Eliminate variables that have zero variance (zeroVar), or variables with near zero variance (nzv).  

In this case, we'll remove any near zero variance variables.

```{r, comment=NA}
nzvCount <- sum(nearZero$nzv)
if (nzvCount > 0)
{
  trainSet <- trainSet[, !nearZero$nzv]  
  testSet  <- testSet[,  !nearZero$nzv]  # make same change to testSet
}
```

#### Remove variables with high correlation to others

```{r, comment=NA}
HIGH_CORRELATION_CUTOFF <- 0.80
corMatrix <- trainSet %>% select(-type) %>% cor   # create correlation matrix

corHigh <- findCorrelation(corMatrix, HIGH_CORRELATION_CUTOFF)
if (length(corHigh) > 0)
{
  cat("Removing highly-correlated variable(s): ", names(trainSet)[corHigh])
  trainSet <- trainSet[, -corHigh]  
  testSet  <- testSet[,  -corHigh]  
}
```

#### Remove variables with linear dependencies

QR decomposition is used to determine if the matrix is full rank.  Sets of columns that are involved in the dependencies are identified.

```{r, comment=NA}
linearCombos <- findLinearCombos(trainSet %>% select(-type))
if (length(linearCombos$remove) > 0)
{
  cat("Removing linear dependencies: ", names(trainSet)[linearCombos$remove])
  trainSet <- trainSet[, -linearCombos$remove]  
  testSet  <- testSet[,  -linearCombos$remove]  
}
```

### Setup parallel processing

```{r, comment=NA}
rCluster <- makePSOCKcluster(6)   # use 6 cores
registerDoParallel(rCluster)  
```

## Transformation

[This source](https://machinelearningmastery.com/pre-process-your-dataset-in-r/) suggests applying ICA for Naive Bayes.

```{r, comment=NA}
preprocessParms <- preProcess(trainSet %>% select(-type),
                              method=c("center", "scale", "ica"),
                              n.comp=5)
print(preprocessParms)
```

```{r, comment=NA}
transformTrain <- predict(preprocessParms, trainSet %>% select(-type))
transformTrain <- cbind(transformTrain, type=trainSet$type)
summary(transformTrain)
```

Apply same transformation to testSet

```{r, comment=NA}
transformTest <- predict(preprocessParms, testSet %>% select(-type))
transformTest <- cbind(transformTest, type=testSet$type)
summary(transformTest)
```

## Naive Bayes with ICA transform

```{r, comment=NA}
set.seed(29)
CVfolds   <-  5  # 5-fold cross validation (not enough data for 10 fold here)
CVrepeats <- 10  # repeat 10 times

trainControlParms <- trainControl(method = "repeatedcv",  # repeated cross validation
                                  number  = CVfolds,    
                                  repeats = CVrepeats,
                                  classProbs = TRUE,        # Estimate class probabilities
                                  summaryFunction = defaultSummary)

fit <- train(type ~ ., data=transformTrain,
             method = "nb",
             metric = "Kappa",
             trControl = trainControlParms)

stopCluster(rCluster)
```

```{r, comment=NA}
print(fit$finalModel)
```

```{r, comment=NA}
plot(fit$finalModel)
```

```{r, comment=NA}
print(fit)
```

### Variable Importance

See ?varImp

```{r, comment=NA, fig.width=8, fig.height=6}
plot( varImp(fit), main="Variable Importance" )
```

### Results on Train Set (In Sample)

Overly optimistic results for generalization

```{r, comment=NA}
options(width=120)
InSample  <- predict(fit, newdata=transformTrain)
InSampleConfusion <- confusionMatrix(transformTrain$type, InSample)
print(InSampleConfusion)   
```

### Results on Test Set (Out of Sample)

More realistic results on predictions with new data

```{r, comment=NA}
options(width=120)
OutOfSample  <- predict(fit, newdata=transformTest)
confusion <- confusionMatrix(transformTest$type, OutOfSample)
print(confusion)   
```

```{r, comment=NA, echo=FALSE}
time.2 <- Sys.time()
processingTime <- paste("Processing time:", sprintf("%.1f",
                        as.numeric(difftime(time.2, time.1, units="secs"))), "sec\n")
```

`r processingTime`
`r format(time.2, "%Y-%m-%d %H:%M:%S")`   

## References

* [A gentle introduction to NaÃ¯ve Bayes classification using R](https://eight2late.wordpress.com/2015/11/06/a-gentle-introduction-to-naive-bayes-classification-using-r/), Eight to Late, 2015.

* [Get Your Data Ready For Machine Learning in R with Pre-Processing](https://machinelearningmastery.com/pre-process-your-dataset-in-r/), Jason Brownlee, 2016.

